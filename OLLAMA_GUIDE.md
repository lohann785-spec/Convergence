# üöÄ GUIDE OLLAMA - LLM LOCAL GRATUIT

## ‚úÖ POURQUOI OLLAMA?

- ‚úÖ **GRATUIT** - Pas de cl√© API, pas de frais
- ‚úÖ **LOCAL** - Tourne sur votre machine
- ‚úÖ **RAPIDE** - Pas de latence r√©seau
- ‚úÖ **PRIV√â** - Les donn√©es ne quittent pas votre PC
- ‚úÖ **HORS-LIGNE** - Fonctionne sans internet apr√®s le t√©l√©chargement

## üì• INSTALLATION (5 MIN)

### 1. T√©l√©charger Ollama

Aller sur: **https://ollama.ai** (ou https://ollama.com)

T√©l√©charger pour votre OS:
- **Windows** ‚Üí ollama-windows.exe
- **Mac** ‚Üí ollama-mac.zip
- **Linux** ‚Üí curl script

### 2. Installer et lancer

```bash
# Sur Windows:
# 1. Double-cliquer l'installateur
# 2. Ollama d√©marre automatiquement

# V√©rifier que √ßa marche:
curl http://localhost:11434/api/tags
```

### 3. T√©l√©charger un mod√®le

Choisir 1 mod√®le (3-5 GB chacun):

```bash
# Mistral (Recommand√© - tr√®s bon rapport qualit√©/vitesse)
ollama pull mistral

# Ou Llama2 (Plus lent, meilleure qualit√©)
ollama pull llama2

# Ou Neural-Chat (Optimis√© pour conversations)
ollama pull neural-chat

# Ou Orca-Mini (Petit, rapide)
ollama pull orca-mini
```

## ‚úÖ V√âRIFIER QUE √áA MARCHE

```bash
# Test simple
curl -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d '{
  "model": "mistral",
  "prompt": "Bonjour, comment allez-vous?",
  "stream": false
}'

# Devrait r√©pondre avec du texte g√©n√©r√©
```

## üîß CONFIGURER L'APP

**Local (.env.local)** - D√©j√† configur√©! ‚úÖ

```
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=mistral
```

**Vercel (optionnel)**

Si vous voulez utiliser Ollama en production sur Vercel:
```
OLLAMA_URL=https://your-ollama-server.com
OLLAMA_MODEL=mistral
```

(Vous devriez h√©berger Ollama sur votre serveur ou Docker)

## üöÄ D√âMARRER L'APP

```bash
# 1. S'assurer que Ollama tourne
ollama serve

# 2. Dans un autre terminal
cd convergence-saa-s-app
pnpm dev

# 3. Ouvrir http://localhost:3000
# 4. Tester "Generate Code"
```

## üìä PERFORMANCE

| Mod√®le | Taille | Qualit√© | Vitesse | RAM |
|--------|--------|---------|---------|-----|
| mistral | 4.1 GB | Tr√®s bon | Rapide | 8 GB |
| llama2 | 3.8 GB | Excellent | Moyen | 12 GB |
| neural-chat | 4.1 GB | Bon | Rapide | 8 GB |
| orca-mini | 1.3 GB | Bon | Tr√®s rapide | 4 GB |

**Ma recommandation:** Commencer avec `mistral` (bon √©quilibre qualit√©/vitesse)

## üíæ ESPACE DISQUE

- Mistral: ~4 GB
- Llama2: ~4 GB
- Orca-mini: ~1.3 GB

**Total avec app:** ~5-6 GB

## ‚ö†Ô∏è ERREURS COURANTES

### "Connection refused on localhost:11434"
```
Solution: Ollama n'est pas lanc√©
‚Üí Ouvrir terminal et taper: ollama serve
```

### "Model not found: mistral"
```
Solution: Le mod√®le n'est pas t√©l√©charg√©
‚Üí ollama pull mistral
‚Üí Attendre le t√©l√©chargement (~5 min)
```

### "Response too slow" ou timeout
```
Solution: Votre PC est trop lent
‚Üí Utiliser orca-mini (plus petit, plus rapide)
‚Üí Ou utiliser Groq API (gratuit, cloud)
```

### "Out of memory"
```
Solution: Pas assez de RAM
‚Üí Fermer autres apps
‚Üí Ou utiliser Groq API gratuitement
```

## üîÑ CHANGER DE MOD√àLE

```bash
# 1. T√©l√©charger nouveau mod√®le
ollama pull llama2

# 2. √âditer .env.local
OLLAMA_MODEL=llama2

# 3. Red√©marrer l'app
# pnpm dev
```

## üìö MOD√àLES DISPONIBLES

```bash
# T√©l√©charger plusieurs mod√®les
ollama pull mistral
ollama pull llama2
ollama pull neural-chat
ollama pull orca-mini

# Voir les mod√®les t√©l√©charg√©s
ollama list

# Supprimer un mod√®le
ollama rm mistral
```

## üåê ALTERNATIVE: GROQ (Gratuit sur Cloud)

Si Ollama est trop lent ou que votre PC n'a pas assez de ressources:

```bash
# 1. Aller sur https://console.groq.com
# 2. Cr√©er un compte
# 3. Cr√©er une cl√© API
# 4. Utiliser groq-sdk (plus rapide que OpenAI!)

npm install groq-sdk
```

Puis changer le code pour utiliser Groq:

```typescript
import Groq from 'groq-sdk'

const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY,
})

const message = await groq.messages.create({
  model: 'mixtral-8x7b-32768',
  messages: [{ role: 'user', content: prompt }],
})
```

**Groq:**
- ‚úÖ Gratuit (limites gen/mois)
- ‚úÖ Super rapide (cloud)
- ‚úÖ Pas d'installation
- ‚ùå N√©cessite internet

## üéØ RECOMMANDATION FINALE

**Pour d√©veloppement local:** Ollama + Mistral
**Pour production gratuit:** Groq API
**Pour vraiment pas cher:** OpenAI (plus flexible, mais paie $)

## üìû SUPPORT

- **Ollama Docs:** https://github.com/ollama/ollama
- **Ollama Web UI:** https://github.com/open-webui/open-webui
- **Groq Docs:** https://console.groq.com/docs

---

**C'est bon! Ollama est 100% gratuit et fonctionne hors-ligne. √Ä vous de jouer! üöÄ**
